# SimpleBench - Research Document

## Problem Statement

The existing Rust benchmarking ecosystem (Criterion, cargo bench) has severe UX issues:
- Running `cargo bench` prints 500+ "ignored test" messages (unit tests being run in bench mode)
- Complex output with statistical analysis that's overkill for simple regression detection
- No straightforward way to fail CI on performance regressions
- Overly complex configuration and usage patterns

**Goal:** Create a simple, ergonomic microbenchmarking tool for pre-commit performance regression checks.

---

## Design Requirements

### User Experience
```rust
#[cfg(mbench)] 
mod bench {
    #[mbench]
    fn populate_world() {
        // benchmark code here
    }
}
```

**Command:** `cargo simplebench`

**Output:**
```
a_benchmark failed - 15% slower than allowed variance!
b_benchmark stable (within allowed variance)
c_benchmark 8% faster than last run!
```

### Technical Requirements
1. **Minimal Cargo.toml changes** - users shouldn't need complex configuration
2. **Single unified test runner binary** - all benchmarks in one executable for efficient scheduling
3. **Machine-specific baselines** - different machines (laptop/desktop/CI) tracked separately
4. **Historical tracking** - JSON files storing past runs for regression detection
5. **Configurable budgets** - both global and per-benchmark performance budgets

---

## Key Research Findings

### 1. Why cargo bench Has Bad UX

**Problem:** When you run `cargo bench`, Cargo:
1. Compiles your library with libtest as a benchmark harness (to handle `#[bench]` functions)
2. Runs it with `--bench` flag
3. Since you have no `#[bench]` functions (they're nightly-only), libtest reports all `#[test]` functions as "ignored"
4. Then runs your actual benchmarks

**Workarounds we rejected:**
- `cargo bench --bench <specific_name>` - tedious, defeats the purpose
- `cargo nextest run --benches` - only runs in "test mode" (1 iteration, no real perf data)
- Shell script filtering - hacky, doesn't solve the root problem
- `[lib] bench = false` in Cargo.toml - still need per-benchmark binaries

### 2. How Test Frameworks Work

**Key Insight:** Cargo creates **separate test binaries per crate**, not one unified binary.

When you run `cargo test`:
- Each crate becomes its own test binary: `target/debug/deps/ai_core-xxxxx`
- libtest is linked into each binary
- Each binary discovers its own `#[test]` functions at compile time

**Tools like nextest:**
- Don't create unified binaries
- Instead, they:
  1. List all tests from all binaries (`./binary --list`)
  2. Build a work queue of individual tests
  3. Schedule tests across cores
  4. Invoke binaries with specific test names: `./binary test_name`
  5. Each invocation runs ONE test then exits

**Why this is suboptimal for SimpleBench:**
- Complex orchestration (master process → many worker processes)
- Inter-process communication overhead
- Can't easily share setup/teardown across benchmarks
- Workaround architecture rather than proper solution

### 3. Creating a Unified Test Runner

**Is it possible?** YES. Here's how:

#### Step 1: Cargo builds the workspace
```bash
cargo build --release
```
This produces rlib files: `target/release/deps/libai_core-xxxxx.rlib`

#### Step 2: Generate a runner binary
```rust
// Generated by cargo-simplebench
extern crate ai_core;
extern crate ai_common;
extern crate physics_engine;
// ... all workspace crates

fn main() {
    simplebench_runtime::main();
}
```

#### Step 3: Link all rlibs into one binary
```bash
rustc runner.rs \
  --extern ai_core=target/release/deps/libai_core-xxxxx.rlib \
  --extern ai_common=target/release/deps/libai_common-xxxxx.rlib \
  -L target/release/deps \
  -o target/simplebench-runner
```

**Key Points:**
- rlibs contain all metadata and dependencies already resolved by Cargo
- We're not fighting Cargo, just using its output differently
- rustc handles all the linking complexity
- The "unstable format" concern is overblown - Firefox does exactly this

### 4. Benchmark Registration with Inventory

**Use the `inventory` crate for compile-time registration:**

```rust
// simplebench-macros/src/lib.rs
#[proc_macro_attribute]
pub fn mbench(_attr: TokenStream, item: TokenStream) -> TokenStream {
    let func = parse_macro_input!(item as ItemFn);
    let func_name = &func.sig.ident;
    
    quote! {
        #func  // Keep original function
        
        inventory::submit! {
            SimpleBench {
                name: stringify!(#func_name),
                module: module_path!(),
                func: #func_name,
            }
        }
    }
    .into()
}
```

**Runtime collection:**
```rust
// simplebench-runtime/src/lib.rs
pub fn main() {
    for bench in inventory::iter::<SimpleBench>() {
        run_benchmark(bench);
    }
}
```

When we compile the unified runner with all crates linked, inventory automatically collects all registered benchmarks across all crates.

### 5. Measurement Strategy

**Challenge:** Timer overhead can distort results for fast functions.

**Measurement on Linux:** `Instant::now()` via `clock_gettime(CLOCK_MONOTONIC)` = ~29ns per call

**Solution:** Batch timing to amortize overhead
- 100 samples for percentile calculation
- Each sample: time 100 iterations together, divide by 100
- Total: 10,000 function invocations
- Timer overhead: 100 calls × 29ns = 2.9μs (negligible for realistic workloads)

**Best Practice:** Benchmark realistic workloads (10,000 entities, not 2)
- Makes timer overhead negligible
- More representative of actual performance
- Avoids artificial micro-loop problems

### 6. Configuration Design

**Global config in Cargo.toml:**
```toml
[package.metadata.simplebench]
max_regression = 0.10  # 10% regression fails
percentile = "p99"
iterations = 10000
samples = 100
```

**Per-benchmark override via attribute:**
```rust
#[mbench(max_regression = 0.05, percentile = "p90")]
fn critical_path() { }
```

**Reading config:**
```rust
use cargo_metadata::MetadataCommand;

let metadata = MetadataCommand::new().exec()?;
let config = metadata.packages[0]
    .metadata
    .get("simplebench")
    .and_then(|v| serde_json::from_value::<Config>(v.clone()).ok());
```

### 7. Historical Data Storage

**Directory structure:**
```
.benches/
  <machine_name>/
    <crate_name>/
      <benchmark_name>.json
```

**JSON format:**
```json
[
  {
    "timestamp": "2025-12-08T10:30:00Z",
    "average": 18.5,
    "p50": 18.2,
    "p90": 19.1,
    "p99": 20.3
  }
]
```

**Machine identification:** Use hostname or allow env var override for CI

**Regression detection:**
- Compare current run against last N runs (configurable)
- Fail if specified percentile exceeds max_regression threshold
- Support both absolute budgets and relative regression limits

---

## Architecture Overview

### Component Breakdown

#### 1. `simplebench-macros` (proc macro crate)
- Provides `#[mbench]` attribute macro
- Registers benchmarks with inventory at compile time
- Extracts configuration from attribute parameters

#### 2. `simplebench-runtime` (library crate)
- Core benchmark execution logic
- Timing and percentile calculation
- JSON output formatting
- Historical data comparison
- Pass/fail determination

#### 3. `cargo-simplebench` (CLI tool)
- Discovers workspace crates via `cargo metadata`
- Builds workspace with `cargo build --release`
- Locates rlib files in target directory
- Generates unified runner.rs
- Invokes rustc to create single test binary
- Executes binary and parses JSON output
- Displays human-readable results

### Data Flow

```
User runs: cargo simplebench
    ↓
CLI discovers workspace structure
    ↓
CLI builds: cargo build --release
    ↓
CLI finds rlibs: lib<crate>-xxxxx.rlib
    ↓
CLI generates: runner.rs (extern all crates)
    ↓
CLI compiles: rustc runner.rs --extern ...
    ↓
CLI executes: ./simplebench-runner --json
    ↓
Runner iterates: inventory::iter::<SimpleBench>()
    ↓
For each benchmark:
  - Load historical data
  - Run timing loop (100 samples × 100 iterations)
  - Calculate percentiles
  - Compare against baseline
  - Output JSON result
    ↓
CLI parses JSON, displays results, exits with code
```

---

## Implementation Challenges & Solutions

### Challenge 1: Finding rlib files
**Problem:** rlibs have hash suffixes: `libai_core-a1b2c3d4.rlib`

**Solution:** 
- Use `cargo metadata` to get exact artifact paths (has `"target"` field with paths)
- Fallback: glob for `lib<crate_name>-*.rlib` and take newest by mtime

### Challenge 2: Dependency ordering
**Problem:** rustc needs dependencies in the right order

**Solution:** 
- `cargo metadata` provides full dependency graph
- Cargo already resolved this when building rlibs
- rustc will handle transitive deps if we pass all rlibs in `-L` path

### Challenge 3: First run (no baseline)
**Problem:** First benchmark run has no historical data

**Solution:**
- On first run, execute benchmark twice
- Use first run as baseline
- Continue normally
- Warn user that first run establishes baseline

### Challenge 4: Cross-compilation
**Problem:** rlibs are architecture-specific

**Solution:**
- Always use same rustc/target as cargo build
- Detect target triple from cargo metadata
- Pass to rustc: `--target <triple>`

### Challenge 5: Parallel execution safety
**Problem:** Some benchmarks may have side effects

**Solution:**
- Run benchmarks serially by default (simpler, safer)
- Document that parallel execution is user's responsibility
- Single binary makes this easy: just use rayon on the benchmark iterator

---

## Why This Approach is Better

### Compared to Criterion:
- ✅ No ignored test spam
- ✅ Simple, clear output
- ✅ Built-in regression detection
- ✅ No complex configuration needed
- ✅ Ergonomic attribute-based API

### Compared to nextest:
- ✅ Actual performance measurement (not test mode)
- ✅ Simpler architecture (one binary, not orchestration)
- ✅ Better parallelization control
- ✅ Less process overhead

### Compared to custom timing tests:
- ✅ Reusable framework
- ✅ Historical tracking built-in
- ✅ Statistical analysis (percentiles)
- ✅ Machine-specific baselines
- ✅ Consistent methodology

---

## Open Questions

1. **Warming up:** Do we need JIT warmup iterations? (Probably not for game engine code)
2. **Outlier detection:** Should we trim outliers before calculating percentiles?
3. **CI integration:** Auto-update baselines on main branch merges?
4. **Reporting:** Generate HTML reports with graphs? (Later feature)
5. **Comparison modes:** Support comparing feature branches against main?

---

## Next Steps (Planning Phase)

1. **Prototype the core runtime** - timing loop, percentile calculation, JSON I/O
2. **Build the attribute macro** - inventory registration
3. **Test manual rustc linking** - verify the rlib approach works
4. **Implement CLI discovery** - cargo metadata parsing, rlib finding
5. **Create code generator** - runner.rs template generation
6. **End-to-end test** - verify full workflow
7. **Polish UX** - error messages, progress indicators
8. **Documentation** - README with examples

---

## References

- Criterion.rs: https://github.com/bheisler/criterion.rs
- cargo-nextest: https://nexte.st/
- inventory crate: https://github.com/dtolnay/inventory
- cargo metadata: https://doc.rust-lang.org/cargo/commands/cargo-metadata.html
- Rust benchmark instability discussion: https://easyperf.net/blog/2019/08/02/Perf-measurement-environment-on-Linux
